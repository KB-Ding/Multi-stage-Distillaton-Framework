[logging]
identifier = minilm_rec_bottle_distill

[train] #train parameters
epoch = 10
batch_size = 64

[data] #data parameters
train_dataset_type = parallel_all_distill_dataset
train_formatter = mse_seprate_formatter
teacher_path = /apdcephfs/share_1157269/karlding/sbert/paraphrase-multilingual-MiniLM-L12-v2

[model] #model parameters
model_name = rec_bottle_distill
student_model_path = /apdcephfs/share_1157269/karlding/sbert/paraphrase-multilingual-MiniLM-L12-v2
emb_ckp_path =  /apdcephfs/share_1157269/karlding/mul_output_train/logs_train/alldata_MiniLM_emb_mse/2021_10_19__15_03_57/best_checkpoint_ep3.pth
repeat = 2
layers_to_keep = 6
scale = 20.0

vocab_size = 250037
embedding_size = 256
pad_token_id = 0
hidden_size = 384
max_position_embeddings = 512
type_vocab_size = 1
layer_norm_eps = 1e-12
hidden_dropout_prob = 0.1

[optim] #optimizer and lr_scheduler parameters
warmup_rate = 0.1
learning_rate =  16e-5
max_grad_norm = 1.0
