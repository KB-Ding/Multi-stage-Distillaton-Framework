[logging]
identifier = xlmr_rec_distill

[train] #train parameters
epoch = 10
batch_size = 64

[data] #data parameters
train_dataset_type = parallel_all_distill_dataset
train_formatter = mse_seprate_formatter
teacher_path = /apdcephfs/share_1157269/karlding/modle_card/paraphrase-xlm-r-multilingual-v1

[model] #model parameters
model_name = rec_distill
student_model_path = /apdcephfs/share_1157269/karlding/modle_card/paraphrase-xlm-r-multilingual-v1
repeat = 4
layers_to_keep = 3
scale = 20.0

[optim] #optimizer and lr_scheduler parameters
warmup_rate = 0.1
learning_rate =  16e-5
max_grad_norm = 1.0