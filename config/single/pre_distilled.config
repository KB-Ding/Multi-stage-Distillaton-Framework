[logging]
identifier = pre_distilled_10ep

[train] #train parameters
epoch = 60
batch_size =128

[data] #data parameters
train_dataset_type = parallel_separate_dataset
train_formatter =  mse_formatter
teacher_path = /apdcephfs/share_1157269/karlding/modle_card/sentence-transformers_paraphrase-distilroberta-base-v1

[model] #model parameters
model_name = pre_distilled
temp_student_path = /apdcephfs/share_1157269/karlding/modle_card/huggingface/xlm-roberta-base
student_model_path = /apdcephfs/share_1157269/karlding/output_train/logs_train/10ep_xlmr_rec_bottle_distill/2022_01_12__11_39_45/best_checkpoint_ep10.pth

repeat = 4
layers_to_keep = 3

vocab_size = 250002
embedding_size = 128
pad_token_id = 1
hidden_size = 768
max_position_embeddings = 514
type_vocab_size = 1
layer_norm_eps = 1e-05
hidden_dropout_prob = 0.1

[optim] #optimizer and lr_scheduler parameters
warmup_rate = 0.1
learning_rate =  16e-5
max_grad_norm = 1.0