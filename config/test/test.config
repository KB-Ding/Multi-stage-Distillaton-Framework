[logging]
identifier = test_sbert

[train] #train parameters
epoch = 5
batch_size = 64

[data] #data parameters
train_dataset_type = parallel_separate_dataset
train_formatter = mse_formatter
teacher_path = /apdcephfs/share_1157269/karlding/modle_card/sentence-transformers_paraphrase-distilroberta-base-v1

[model] #model parameters
model_name = mul_mse
student_model_path = /apdcephfs/share_1157269/karlding/sbert/LaBSE

[optim] #optimizer and lr_scheduler parameters
warmup_rate = 0.1
learning_rate =  16e-5
max_grad_norm = 0.0